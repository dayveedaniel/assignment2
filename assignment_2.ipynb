{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKJLm5VsRFMS"
      },
      "source": [
        "# ASSIGNMENT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ezSVcGKRFMU"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebYOTxJFRFMU"
      },
      "source": [
        "### Data Reading,Exploration and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "7N-lF7dkRFMU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms,models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    # transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "path = '.data/'\n",
        "train_dataset = datasets.Flowers102(root=path,split='train', transform=transform, download=True)\n",
        "test_dataset = datasets.Flowers102(root=path,split='test', transform=transform, download=True)\n",
        "val_dataset = datasets.Flowers102(root=path,split='val', transform=transform, download=True)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5YsDh7RFMV"
      },
      "source": [
        "### Machine learning or Deep learning model defining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHWVPREKRFMV",
        "outputId": "810040a4-1b27-4fbc-aaac-b0d195ab16d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaselineModel(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): Dropout(p=0.3, inplace=False)\n",
              "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): Dropout(p=0.3, inplace=False)\n",
              "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (fc1): Sequential(\n",
              "    (0): Linear(in_features=131072, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (output): Linear(in_features=512, out_features=102, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Define the baseline CNN model\n",
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.features = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                      nn.BatchNorm2d(32),\n",
        "                                      nn.Dropout(p=0.3),\n",
        "\n",
        "                                      nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                      nn.BatchNorm2d(64),\n",
        "                                      nn.Dropout(p=0.3),\n",
        "\n",
        "                                      nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                      nn.BatchNorm2d(128),\n",
        "                                      nn.Dropout(p=0.3)\n",
        "                                      )\n",
        "        self.fc1 = nn.Sequential(nn.Linear(32 * 32 * 128, 512),\n",
        "                                 nn.ReLU()\n",
        "                                 )\n",
        "\n",
        "        self.output = nn.Linear(512, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.output(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        # x = x.view(x.size(0), -1)  # Flatten\n",
        "        # x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "BaselineModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMh8BgwuRFMW"
      },
      "source": [
        "#### Specify loss func and optimizer for base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "jNiprI3_RFMW",
        "outputId": "4a8437cd-e017-40b9-8177-ff2bdd97a169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e638dd87d5d7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using device: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaselineModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mbase_model_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model, optimizer, and loss function\n",
        "device = 'cuda'\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "model = BaselineModel().to(device)\n",
        "base_model_optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "nll_criterion = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USeoyc_BRFMW"
      },
      "source": [
        "### Training model func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QaliSGqRFMW"
      },
      "outputs": [],
      "source": [
        "def training_model(writer:SummaryWriter,criterion,optimizer,model,epochs:int = 10):\n",
        "    # Initialize TensorBoard writer\n",
        "    epochs = 10\n",
        "    # Train the model\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update training loss and accuracy\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = train_correct / len(train_dataset)\n",
        "\n",
        "        # Evaluate the model on the val set\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_labels = []\n",
        "        val_predicted = []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                # Move data to device\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "                val_predicted.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calculate test loss and accuracy\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = val_correct / len(val_dataset)\n",
        "\n",
        "        # Calculate F1 score\n",
        "        test_f1 = f1_score(val_labels, val_predicted, average='macro')\n",
        "\n",
        "        # Log metrics to TensorBoard\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
        "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Val', val_accuracy, epoch)\n",
        "        writer.add_scalar('F1/Val', test_f1, epoch)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "        print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {test_f1:.4f}')\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize Summary writers"
      ],
      "metadata": {
        "id": "JMRMyMJ5RuFN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCNoYNuwR5y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaWFp3i7RFMX"
      },
      "source": [
        "#### Train base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVxSRwQARFMX"
      },
      "outputs": [],
      "source": [
        "training_model(writer=SummaryWriter(),criterion=criterion,optimizer=base_model_optimizer,model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VljevT_-RFMX"
      },
      "source": [
        "## Transfer Learning using VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m1lR8BuRFMX"
      },
      "source": [
        "#### Define VGG model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEIPqcKnRFMX"
      },
      "outputs": [],
      "source": [
        "vgg_model = models.vgg16(pretrained=True)\n",
        "# for param in vgg_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "vgg_model.classifier = nn.Sequential(\n",
        "                        nn.Linear(25088, 256),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(p=0.2),\n",
        "\n",
        "                        nn.Linear(256, 128),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(p=0.2),\n",
        "\n",
        "                        nn.Linear(128, 32),\n",
        "                        nn.LogSoftmax(dim=1))\n",
        "\n",
        "vgg_model = vgg_model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "vgg_optimizer = optim.Adam(vgg_model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "vgg_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS5jWOA8RFMY"
      },
      "source": [
        "#### Train VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKiiUpLqRFMY"
      },
      "outputs": [],
      "source": [
        "training_model(writer=SummaryWriter(),criterion=criterion,optimizer=vgg_optimizer,model=vgg_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptTK_1yRRFMY"
      },
      "source": [
        "## Transfer Learning using ResNet-50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XObRjjTZRFMY"
      },
      "source": [
        "#### Define ResNet-50 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4O_yzMARFMY"
      },
      "outputs": [],
      "source": [
        "resnet_model = models.resnet50()\n",
        "# for param in resnet_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "resnet_model.classifier = nn.Sequential(\n",
        "                        nn.Linear(512, 256),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(p=0.2),\n",
        "\n",
        "                        nn.Linear(256, 128),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(p=0.2),\n",
        "\n",
        "                        nn.Linear(128, 32),\n",
        "                        nn.LogSoftmax(dim=1))\n",
        "\n",
        "resnet_model = resnet_model.to(device)\n",
        "# Define the optimizer\n",
        "resnet_optimizer = optim.Adam(resnet_model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "resnet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkNynNAYRFMZ"
      },
      "source": [
        "#### Train ResNet-50 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmZpCb98RFMZ"
      },
      "outputs": [],
      "source": [
        "training_model(writer=SummaryWriter(),criterion=criterion,optimizer=resnet_optimizer,model=resnet_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqE_us1RFMZ"
      },
      "source": [
        "### Model performance evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhQleNlZRFMZ"
      },
      "source": [
        "##### Testing Model performance with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmcx2CjfRFMZ"
      },
      "outputs": [],
      "source": [
        "def test_model(model, writer:SummaryWriter,criterion):\n",
        "# Evaluate the model on the val set\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        test_correct = 0\n",
        "        test_labels = []\n",
        "        test_predicted = []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                # Move data to device\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "                test_labels.extend(labels.cpu().numpy())\n",
        "                test_predicted.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calculate test loss and accuracy\n",
        "        test_loss /= len(test_loader)\n",
        "        test_accuracy = test_correct / len(test_dataset)\n",
        "\n",
        "        # Calculate F1 score\n",
        "        test_f1 = f1_score(test_labels, test_predicted, average='macro')\n",
        "\n",
        "        # Log metrics to TensorBoard\n",
        "        writer.add_scalar('Loss/Val', test_loss)\n",
        "        writer.add_scalar('Accuracy/Val', test_accuracy)\n",
        "        writer.add_scalar('F1/Val', test_f1)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1: {test_f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49P36OplRFMZ"
      },
      "source": [
        "Baseline model test score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw1aVFWsRFMa"
      },
      "outputs": [],
      "source": [
        "test_model(model,criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lfe0fKRFMa"
      },
      "source": [
        "VGG-16 test score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soKaWLifRFMa"
      },
      "outputs": [],
      "source": [
        "test_model(vgg_model,criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXJgsNYaRFMa"
      },
      "source": [
        "ResNet-50 test score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsaQjEV6RFMa"
      },
      "outputs": [],
      "source": [
        "test_model(resnet_model,criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXQUtGoqRFMa"
      },
      "source": [
        "### Conclusion and possible improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZcRvoiqRFMa"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAKqK4rmRFMa"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqALHbX4RFMa"
      },
      "source": [
        "To cluster images of flowers into distinct groups based on their visual features using unsupervised\n",
        "learning techniques. The goal is to utilize a pre-trained convolutional neural network (CNN) to\n",
        "extract features from the images and then apply a clustering algorithm to categorize the flowers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwkvK1xcRFMa"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acae8sIaRFMa"
      },
      "outputs": [],
      "source": [
        "# Extract features from the flower images\n",
        "features = []\n",
        "for images, _ in train_loader:\n",
        "    images = images.to(device)\n",
        "    feature_vectors = resnet_model(images)\n",
        "    features.append(feature_vectors.detach().cpu().numpy())\n",
        "\n",
        "features = np.concatenate(features, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsnxMhdHRFMa"
      },
      "source": [
        "### Clustering: Cluster the extracted features using K-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTnq1gF-RFMa"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Perform K-means clustering on the extracted features\n",
        "n_clusters = 10\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(features)\n",
        "\n",
        "# Reduce the feature dimensionality using PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NTDqehPRFMb"
      },
      "source": [
        "### Clusters visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVBraRbSRFMb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the clustered data points\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(pca_features[:, 0], pca_features[:, 1], c=cluster_labels, cmap='viridis')\n",
        "plt.title('Flower Image Clusters')\n",
        "plt.xlabel('PCA Dimension 1')\n",
        "plt.ylabel('PCA Dimension 2')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}